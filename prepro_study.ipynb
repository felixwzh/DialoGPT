{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## study the preprocess code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  Copyright (c) Microsoft Corporation. \n",
    "#  Licensed under the MIT license. \n",
    "\"\"\"\n",
    "preprocess input data into feature and stores binary as python shelve DB\n",
    "each chunk is gzipped JSON string\n",
    "\"\"\"\n",
    "import argparse\n",
    "import gzip\n",
    "import json\n",
    "import subprocess as sp\n",
    "import shelve\n",
    "import os\n",
    "from os.path import dirname, exists, join\n",
    "\n",
    "import torch\n",
    "from lsp_model import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from env import END_OF_TEXT_TOKEN\n",
    "from gpt2_training.train_utils import InputFeatures_train as InputFeatures\n",
    "\n",
    "\n",
    "def _get_file_len(corpus):\n",
    "    n_line = int(sp.check_output(f\"wc -l {corpus}\".split(),\n",
    "                                 universal_newlines=True).split()[0])\n",
    "    return n_line\n",
    "\n",
    "\n",
    "def _norm_text(text):\n",
    "    w, *toks = text.strip().split()\n",
    "    try:\n",
    "        w = float(w)\n",
    "    except Exception:\n",
    "        toks = [w] + toks\n",
    "        w = 1.0\n",
    "    return w, ' '.join(toks)\n",
    "\n",
    "\n",
    "def _get_inputs_from_text(text, tokenizer):\n",
    "    srcs, tgt = text.strip().split('\\t')\n",
    "    weights = []\n",
    "    inputs = []\n",
    "    for src in srcs.split(' EOS '):\n",
    "        src_weight, src = _norm_text(src)\n",
    "        context_id = tokenizer.encode(src)\n",
    "        weights.append(src_weight)\n",
    "        inputs.append(context_id)\n",
    "    tgt_weight, tgt = _norm_text(tgt)\n",
    "    if tgt_weight != 0:\n",
    "        response_id = tokenizer.encode(tgt)\n",
    "        weights.append(tgt_weight)\n",
    "        inputs.append(response_id)\n",
    "    return weights, inputs\n",
    "\n",
    "\n",
    "def _make_features(id_, weights, inputs, tokenizer, max_len):\n",
    "    end_of_text_id = tokenizer.encoder[END_OF_TEXT_TOKEN]\n",
    "    features = []\n",
    "    sents = []\n",
    "    ws = []\n",
    "    len_ = 0\n",
    "    i = 0\n",
    "    if True:\n",
    "        if len(weights)==2 and weights[-1]>0:\n",
    "            persona_id=int(weights[-1])\n",
    "    for ids, w in zip(inputs, weights):\n",
    "        if len(ids) > max_len:\n",
    "            if len(sents) >= 2:\n",
    "                if True:\n",
    "\n",
    "                    feat = _make_feature(id_ + i, sents, ws, end_of_text_id,persona_id)\n",
    "                else:\n",
    "                    feat = _make_feature(id_ + i, sents, ws, end_of_text_id)\n",
    "                if feat is not None:\n",
    "                    features.append(feat)\n",
    "                    i += 1\n",
    "            len_ = 0\n",
    "            sents = []\n",
    "            ws = []\n",
    "            continue\n",
    "        elif len_ > max_len:\n",
    "            if True:\n",
    "                feat = _make_feature(id_ + i, sents, ws, end_of_text_id,persona_id)\n",
    "            else:\n",
    "                feat = _make_feature(id_ + i, sents, ws, end_of_text_id)\n",
    "            if feat is not None:\n",
    "                features.append(feat)\n",
    "                i += 1\n",
    "            len_ = len(sents[-1]) + 1\n",
    "            sents = sents[-1:]\n",
    "            ws = ws[-1:]\n",
    "        len_ += (len(ids) + 1)\n",
    "        sents.append(ids)\n",
    "        ws.append(w)\n",
    "    if len(sents) >= 2:\n",
    "        if True:\n",
    "            feat = _make_feature(id_ + i, sents, ws, end_of_text_id,persona_id)\n",
    "        else:\n",
    "            feat = _make_feature(id_ + i, sents, ws, end_of_text_id)\n",
    "\n",
    "        if feat is not None:\n",
    "            features.append(feat)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def _make_feature(id_, sents, ws, eos,persona_id=None):\n",
    "    if all(w == 0 for w in ws[1:]):\n",
    "        return None\n",
    "    input_ids = [i for s in sents for i in s+[eos]][:-1]\n",
    "    lm_labels = []\n",
    "    weights = []\n",
    "    token_type_ids = []  # this becomes round ids\n",
    "    for i, (s, w) in enumerate(zip(sents, ws)):\n",
    "        if i == 0:\n",
    "            lm_labels += [-1] * len(s)\n",
    "            weights += [0.0] * len(s)\n",
    "            token_type_ids += [0] * len(s)\n",
    "            continue\n",
    "\n",
    "        token_type_ids += [i] * (len(s) + 1)\n",
    "        if w == 0.0:\n",
    "            lm_labels += [-1] * (len(s) + 1)\n",
    "            weights += [0.0] * (len(s) + 1)\n",
    "        else:\n",
    "            lm_labels += (s + [eos])\n",
    "            weights += [w] * (len(s) + 1)\n",
    "\n",
    "    # handle trailing -1's\n",
    "    i = len(lm_labels) - 1\n",
    "    while i >= 0:\n",
    "        if lm_labels[i] != -1:\n",
    "            break\n",
    "        i -= 1\n",
    "    input_ids = input_ids[:i+1]\n",
    "    lm_labels = lm_labels[:i+1]\n",
    "    weights = weights[:i+1]\n",
    "    token_type_ids = token_type_ids[:i+1]\n",
    "\n",
    "    # pad to multiples of 8\n",
    "    while len(input_ids) % 8 != 0:\n",
    "        input_ids.append(0)\n",
    "        token_type_ids.append(0)\n",
    "        lm_labels.append(-1)\n",
    "        weights.append(0.0)\n",
    "\n",
    "    position_ids = list(range(len(input_ids)))\n",
    "    assert (len(input_ids) == len(position_ids) == len(token_type_ids)\n",
    "            == len(lm_labels) == len(weights))\n",
    "    assert len(input_ids) % 8 == 0\n",
    "    if len(input_ids) == 0:\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "    feature = InputFeatures(id_, input_ids, position_ids, token_type_ids,\n",
    "                            lm_labels, weights,persona_id=persona_id)\n",
    "    return feature\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    toker = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    attrs = []\n",
    "    if args.reverse:\n",
    "        attrs.append('reverse')\n",
    "    if args.two_turn:\n",
    "        attrs.append('2turn')\n",
    "    if attrs:\n",
    "        db_path = (f'{args.corpus[:-4]}.{args.max_seq_len}len.'\n",
    "                   f'{\".\".join(attrs)}.db/db')\n",
    "    else:\n",
    "        db_path = f'{args.corpus[:-4]}.{args.max_seq_len}len.db/db'\n",
    "    if exists(dirname(db_path)):\n",
    "        raise ValueError('Found existing DB, please backup')\n",
    "    else:\n",
    "        os.makedirs(dirname(db_path))\n",
    "    with open(args.corpus, \"r\", encoding=\"utf-8\") as reader, \\\n",
    "            shelve.open(db_path, 'n') as db:\n",
    "        chunk = []\n",
    "        n_chunk = 0\n",
    "        n_example = 0\n",
    "        for line in tqdm(reader, total=_get_file_len(args.corpus)):\n",
    "            try:\n",
    "                if len(chunk) >= args.chunk_size:\n",
    "                    # save and renew chunk\n",
    "                    db[f'chunk_{n_chunk}'] = gzip.compress(\n",
    "                        json.dumps(chunk[:args.chunk_size]).encode('utf-8'))\n",
    "                    chunk = chunk[args.chunk_size:]\n",
    "                    n_chunk += 1\n",
    "\n",
    "                weights, inputs = _get_inputs_from_text(line, toker)\n",
    "                # at this point, the weights are still what we need. \n",
    "                \n",
    "                if args.reverse:\n",
    "                    weights = list(reversed(weights))\n",
    "                    inputs = list(reversed(inputs))\n",
    "                if args.two_turn:\n",
    "                    weights = weights[:2]\n",
    "                    inputs = inputs[:2]\n",
    "                if len(weights) < 2:\n",
    "                    continue\n",
    "                features = _make_features(n_example, weights, inputs,\n",
    "                                          toker, args.max_seq_len)\n",
    "                for feature in features:\n",
    "                    chunk.append(vars(feature))\n",
    "                    n_example += 1\n",
    "            except Exception as e:\n",
    "                print('!!! prepro exception !!!', e)\n",
    "                continue\n",
    "        # save last chunk\n",
    "        db[f'chunk_{n_chunk}'] = gzip.compress(\n",
    "            json.dumps(chunk).encode('utf-8'))\n",
    "    # save relevant information to reproduce\n",
    "    meta = {'n_example': n_example,\n",
    "            'chunk_size': args.chunk_size,\n",
    "            'max_seq_len': args.max_seq_len,\n",
    "            'reverse': args.reverse,\n",
    "            'two_turn': args.two_turn}\n",
    "    with open(join(dirname(db_path), 'meta.json'), 'w') as writer:\n",
    "        json.dump(meta, writer, indent=4)\n",
    "    torch.save(toker, join(dirname(db_path), 'tokenizer.pt'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toker = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer=toker\n",
    "text=\"0.0 what ' s it like to be attractive ? to like walk into a room and have girls actually looking at you ?\t0 reddit is definitely the best place to ask this question\"\n",
    "weights, inputs = _get_inputs_from_text(text, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0],\n",
       " [[10919,\n",
       "   705,\n",
       "   264,\n",
       "   340,\n",
       "   588,\n",
       "   284,\n",
       "   307,\n",
       "   10966,\n",
       "   5633,\n",
       "   284,\n",
       "   588,\n",
       "   2513,\n",
       "   656,\n",
       "   257,\n",
       "   2119,\n",
       "   290,\n",
       "   423,\n",
       "   4813,\n",
       "   1682,\n",
       "   2045,\n",
       "   379,\n",
       "   345,\n",
       "   5633]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights,inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c9e88f57f16c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_example\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "n_example=0\n",
    "\n",
    "features = _make_features(n_example, weights, inputs,toker, 128)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_id:\t"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b92b8a438cb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv_id:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input_ids:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print('conv_id:',end='\\t')\n",
    "print(features.conv_id)\n",
    "\n",
    "print('input_ids:',end='\\t')\n",
    "print(features.input_ids )\n",
    "\n",
    "print('position_ids:',end='\\t')\n",
    "print(features.position_ids )\n",
    "\n",
    "print('token_type_ids:',end='\\t')\n",
    "print(features.token_type_ids )\n",
    "\n",
    "print('lm_labels:',end='\\t')\n",
    "print(features.lm_labels )\n",
    "\n",
    "print('weights:',end='\\t')\n",
    "print(features.weights )\n",
    "\n",
    "print('persona_id:',end='\\t')\n",
    "print(features.persona_id )\n",
    "print(features.input_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:\t[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "text=\"0.0 if you could have one ' do over ' in your life , what would you do differently ?\t34 find dif   ds     ways to fund myself - no matter how desperate ,f f f  there was always a better way .\"\n",
    "weights, inputs = _get_inputs_from_text(text, tokenizer)\n",
    "n_example=0\n",
    "\n",
    "features = _make_features(n_example, weights, inputs,toker, 128)[0]\n",
    "print('weights:',end='\\t')\n",
    "print(features.weights )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check this one: \n",
    "\n",
    "#https://github.com/huggingface/transformers/tree/v0.6.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the worst thing someone has confessed to you ?<|endoftext|>someone confessed to killing someone to me back in the 70s when i was at a bar like digit years ago .<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "tokens=[10919,   318,   262,  5290,  1693,   345,  1683,   750,  5633, 50256,\n",
    "          7890, 22967,   764,     0,     0,     0]\n",
    "tokens=[10919,   318,   262,  5290,  1517,  2130,   468, 25623,   284,   345,\n",
    "          5633, 50256, 46248, 25623,   284,  5170,  2130,   284,   502,   736,\n",
    "           287,   262,  4317,    82,   618,  1312,   373,   379,   257,  2318,\n",
    "           588, 16839,   812,  2084,   764, 50256]\n",
    "print(toker.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "token_ids[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=torch.tensor([[[1,1,1,1],[1,1,1,1],[1,1,1,1]],[[1,1,1,1],[1,1,1,1],[1,1,1,1]]]) # 2x3x4\n",
    "a=torch.tensor([[0,1,1],[1,1,0]]).reshape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 1]), torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size(),b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.],\n",
       "         [1.],\n",
       "         [1.]],\n",
       "\n",
       "        [[1.],\n",
       "         [1.],\n",
       "         [0.]]], dtype=torch.float16)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.type(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0],\n",
       "         [1, 1, 1, 1],\n",
       "         [1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1, 1],\n",
       "         [1, 1, 1, 1],\n",
       "         [0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model_name_or_path MODEL_NAME_OR_PATH]\n",
      "                             [--seed SEED] [--max_seq_length MAX_SEQ_LENGTH]\n",
      "                             [--skip_eval] [--init_checkpoint INIT_CHECKPOINT]\n",
      "                             [--train_input_file TRAIN_INPUT_FILE]\n",
      "                             [--eval_input_file EVAL_INPUT_FILE]\n",
      "                             [--continue_from CONTINUE_FROM]\n",
      "                             [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--eval_batch_size EVAL_BATCH_SIZE]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--num_optim_steps NUM_OPTIM_STEPS]\n",
      "                             [--valid_step VALID_STEP]\n",
      "                             [--warmup_proportion WARMUP_PROPORTION]\n",
      "                             [--warmup_steps WARMUP_STEPS]\n",
      "                             [--normalize_data NORMALIZE_DATA] [--fp16 FP16]\n",
      "                             [--lr_schedule {noam,noamwd,BERT,None}]\n",
      "                             [--loss_scale LOSS_SCALE]\n",
      "                             [--no_token_id NO_TOKEN_ID]\n",
      "                             [--output_dir OUTPUT_DIR] [--log_dir LOG_DIR]\n",
      "                             [--pbar PBAR] [--local_rank LOCAL_RANK]\n",
      "                             [--config CONFIG]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/felix/.local/share/jupyter/runtime/kernel-e2863679-ebc9-4a81-b0cf-3916de736ca0.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#  Copyright (c) Microsoft Corporation. \n",
    "#  Licensed under the MIT license. \n",
    "'''\n",
    " * @Desc: train GPT2 from scratch/ fine tuning.\n",
    "          Modified based on Huggingface GPT-2 implementation\n",
    "'''\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import tqdm\n",
    "import datetime\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from os.path import join\n",
    "from torch.distributed import get_rank, get_world_size\n",
    "\n",
    "from lsp_model import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, Adam\n",
    "from gpt2_training.train_utils import load_model, boolean_string, set_lr, get_eval_list_same_length\n",
    "from gpt2_training.eval_utils import eval_model_loss\n",
    "\n",
    "from data_loader import BucketingDataLoader, DynamicBatchingLoader, DistributedBucketingDataLoader\n",
    "\n",
    "\n",
    "from gpt2_training.distributed import all_reduce_and_rescale_tensors, all_gather_list\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "    datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "INF = 100000000\n",
    "CACHE_EMPTY_STEP = 10000\n",
    "EVAL_STEP = 100000\n",
    "\n",
    "#########################################################################\n",
    "# Prepare Parser\n",
    "##########################################################################\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_name_or_path', type=str,\n",
    "                    help='pretrained model name or path to local checkpoint')\n",
    "parser.add_argument(\"--seed\", type=int, default=42)\n",
    "parser.add_argument(\"--max_seq_length\", type=int, default=128)\n",
    "\n",
    "parser.add_argument(\"--skip_eval\", action='store_true',\n",
    "                    help='If true, skip evaluation.')\n",
    "parser.add_argument(\"--init_checkpoint\", type=str)\n",
    "parser.add_argument(\"--train_input_file\", type=str)\n",
    "parser.add_argument(\"--eval_input_file\", type=str)\n",
    "parser.add_argument(\"--continue_from\", type=int, default=0)\n",
    "\n",
    "parser.add_argument(\"--train_batch_size\", type=int, default=4,\n",
    "                    help=\"batch size now means per GPU per step\")\n",
    "parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=2,\n",
    "                    help=\"to increase effective batch size \"\n",
    "                         \"and reduce synchronization\")\n",
    "parser.add_argument(\"--eval_batch_size\", type=int, default=4)\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=1e-5)\n",
    "parser.add_argument(\"--num_optim_steps\", type=int, default=1000000,\n",
    "                    help=\"new API specifies num update steps\")\n",
    "parser.add_argument(\"--valid_step\", type=int, default=10000,\n",
    "                    help=\"how many optim steps between validations\")\n",
    "parser.add_argument(\"--warmup_proportion\", type=float, default=0.1)\n",
    "parser.add_argument(\"--warmup_steps\", type=int, default=16000)\n",
    "\n",
    "parser.add_argument(\"--normalize_data\", type=boolean_string, default=True)\n",
    "parser.add_argument(\"--fp16\", type=boolean_string, default=True)\n",
    "parser.add_argument(\"--lr_schedule\", type=str,\n",
    "                    choices=['noam', 'noamwd', 'BERT', 'None'], default='noam')\n",
    "parser.add_argument(\"--loss_scale\", type=float, default=0)\n",
    "parser.add_argument(\"--no_token_id\", type=boolean_string, default=True)\n",
    "\n",
    "parser.add_argument(\"--output_dir\", type=str)\n",
    "parser.add_argument(\"--log_dir\", type=str)\n",
    "parser.add_argument('--pbar', type=boolean_string, default=True, help='turn on progress bar')\n",
    "\n",
    "# distributed\n",
    "parser.add_argument('--local_rank', type=int, default=-1,\n",
    "                    help='for torch.distributed')\n",
    "parser.add_argument('--config', help='JSON config file')\n",
    "\n",
    "\n",
    "# do normal parsing\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros([5],dtype=torch.long, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_persona_emb_ids=torch.ones(10,dtype=torch.long)\n",
    "token_persona_emb_ids = persona_ids.reshape((-1,1)) * token_persona_emb_ids \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "X_scaled = preprocessing.scale(X_train)\n",
    "\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.9343245538895844e-17"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/felix/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = np.random.randint(25, size=(4, 4))\n",
    "standardized_data = StandardScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  6,  8, 15],\n",
       "       [17, 23, 22,  9],\n",
       "       [12, 23,  3, 10],\n",
       "       [22,  6, 16,  8]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.28280871, -1.        , -0.58275249,  1.67125804],\n",
       "       [ 0.4276029 ,  1.        ,  1.33690278, -0.55708601],\n",
       "       [-0.52262577,  1.        , -1.26834366, -0.18569534],\n",
       "       [ 1.37783158, -1.        ,  0.51419338, -0.92847669]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [ 1.22474487],\n",
       "       [-1.22474487]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StandardScaler().fit_transform(X_train[2,:].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
