{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## study the preprocess code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  Copyright (c) Microsoft Corporation. \n",
    "#  Licensed under the MIT license. \n",
    "\"\"\"\n",
    "preprocess input data into feature and stores binary as python shelve DB\n",
    "each chunk is gzipped JSON string\n",
    "\"\"\"\n",
    "import argparse\n",
    "import gzip\n",
    "import json\n",
    "import subprocess as sp\n",
    "import shelve\n",
    "import os\n",
    "from os.path import dirname, exists, join\n",
    "\n",
    "import torch\n",
    "from lsp_model import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from env import END_OF_TEXT_TOKEN\n",
    "from gpt2_training.train_utils import InputFeatures_train as InputFeatures\n",
    "\n",
    "\n",
    "def _get_file_len(corpus):\n",
    "    n_line = int(sp.check_output(f\"wc -l {corpus}\".split(),\n",
    "                                 universal_newlines=True).split()[0])\n",
    "    return n_line\n",
    "\n",
    "\n",
    "def _norm_text(text):\n",
    "    w, *toks = text.strip().split()\n",
    "    try:\n",
    "        w = float(w)\n",
    "    except Exception:\n",
    "        toks = [w] + toks\n",
    "        w = 1.0\n",
    "    return w, ' '.join(toks)\n",
    "\n",
    "\n",
    "def _get_inputs_from_text(text, tokenizer):\n",
    "    srcs, tgt = text.strip().split('\\t')\n",
    "    weights = []\n",
    "    inputs = []\n",
    "    for src in srcs.split(' EOS '):\n",
    "        src_weight, src = _norm_text(src)\n",
    "        context_id = tokenizer.encode(src)\n",
    "        weights.append(src_weight)\n",
    "        inputs.append(context_id)\n",
    "    tgt_weight, tgt = _norm_text(tgt)\n",
    "    if tgt_weight != 0:\n",
    "        response_id = tokenizer.encode(tgt)\n",
    "        weights.append(tgt_weight)\n",
    "        inputs.append(response_id)\n",
    "    return weights, inputs\n",
    "\n",
    "\n",
    "def _make_features(id_, weights, inputs, tokenizer, max_len):\n",
    "    end_of_text_id = tokenizer.encoder[END_OF_TEXT_TOKEN]\n",
    "    features = []\n",
    "    sents = []\n",
    "    ws = []\n",
    "    len_ = 0\n",
    "    i = 0\n",
    "    for ids, w in zip(inputs, weights):\n",
    "        if len(ids) > max_len:\n",
    "            if len(sents) >= 2:\n",
    "                feat = _make_feature(id_ + i, sents, ws, end_of_text_id)\n",
    "                if feat is not None:\n",
    "                    features.append(feat)\n",
    "                    i += 1\n",
    "            len_ = 0\n",
    "            sents = []\n",
    "            ws = []\n",
    "            continue\n",
    "        elif len_ > max_len:\n",
    "            feat = _make_feature(id_ + i, sents, ws, end_of_text_id)\n",
    "            if feat is not None:\n",
    "                features.append(feat)\n",
    "                i += 1\n",
    "            len_ = len(sents[-1]) + 1\n",
    "            sents = sents[-1:]\n",
    "            ws = ws[-1:]\n",
    "        len_ += (len(ids) + 1)\n",
    "        sents.append(ids)\n",
    "        ws.append(w)\n",
    "    if len(sents) >= 2:\n",
    "        feat = _make_feature(id_ + i, sents, ws, end_of_text_id)\n",
    "        if feat is not None:\n",
    "            features.append(feat)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def _make_feature(id_, sents, ws, eos):\n",
    "    if all(w == 0 for w in ws[1:]):\n",
    "        return None\n",
    "    input_ids = [i for s in sents for i in s+[eos]][:-1]\n",
    "    lm_labels = []\n",
    "    weights = []\n",
    "    token_type_ids = []  # this becomes round ids\n",
    "    for i, (s, w) in enumerate(zip(sents, ws)):\n",
    "        if i == 0:\n",
    "            lm_labels += [-1] * len(s)\n",
    "            weights += [0.0] * len(s)\n",
    "            token_type_ids += [0] * len(s)\n",
    "            continue\n",
    "\n",
    "        token_type_ids += [i] * (len(s) + 1)\n",
    "        if w == 0.0:\n",
    "            lm_labels += [-1] * (len(s) + 1)\n",
    "            weights += [0.0] * (len(s) + 1)\n",
    "        else:\n",
    "            lm_labels += (s + [eos])\n",
    "            weights += [w] * (len(s) + 1)\n",
    "\n",
    "    # handle trailing -1's\n",
    "    i = len(lm_labels) - 1\n",
    "    while i >= 0:\n",
    "        if lm_labels[i] != -1:\n",
    "            break\n",
    "        i -= 1\n",
    "    input_ids = input_ids[:i+1]\n",
    "    lm_labels = lm_labels[:i+1]\n",
    "    weights = weights[:i+1]\n",
    "    token_type_ids = token_type_ids[:i+1]\n",
    "\n",
    "    # pad to multiples of 8\n",
    "    while len(input_ids) % 8 != 0:\n",
    "        input_ids.append(0)\n",
    "        token_type_ids.append(0)\n",
    "        lm_labels.append(-1)\n",
    "        weights.append(0.0)\n",
    "\n",
    "    position_ids = list(range(len(input_ids)))\n",
    "    assert (len(input_ids) == len(position_ids) == len(token_type_ids)\n",
    "            == len(lm_labels) == len(weights))\n",
    "    assert len(input_ids) % 8 == 0\n",
    "    if len(input_ids) == 0:\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "    feature = InputFeatures(id_, input_ids, position_ids, token_type_ids,\n",
    "                            lm_labels, weights)\n",
    "    return feature\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    toker = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    attrs = []\n",
    "    if args.reverse:\n",
    "        attrs.append('reverse')\n",
    "    if args.two_turn:\n",
    "        attrs.append('2turn')\n",
    "    if attrs:\n",
    "        db_path = (f'{args.corpus[:-4]}.{args.max_seq_len}len.'\n",
    "                   f'{\".\".join(attrs)}.db/db')\n",
    "    else:\n",
    "        db_path = f'{args.corpus[:-4]}.{args.max_seq_len}len.db/db'\n",
    "    if exists(dirname(db_path)):\n",
    "        raise ValueError('Found existing DB, please backup')\n",
    "    else:\n",
    "        os.makedirs(dirname(db_path))\n",
    "    with open(args.corpus, \"r\", encoding=\"utf-8\") as reader, \\\n",
    "            shelve.open(db_path, 'n') as db:\n",
    "        chunk = []\n",
    "        n_chunk = 0\n",
    "        n_example = 0\n",
    "        for line in tqdm(reader, total=_get_file_len(args.corpus)):\n",
    "            try:\n",
    "                if len(chunk) >= args.chunk_size:\n",
    "                    # save and renew chunk\n",
    "                    db[f'chunk_{n_chunk}'] = gzip.compress(\n",
    "                        json.dumps(chunk[:args.chunk_size]).encode('utf-8'))\n",
    "                    chunk = chunk[args.chunk_size:]\n",
    "                    n_chunk += 1\n",
    "\n",
    "                weights, inputs = _get_inputs_from_text(line, toker)\n",
    "                if args.reverse:\n",
    "                    weights = list(reversed(weights))\n",
    "                    inputs = list(reversed(inputs))\n",
    "                if args.two_turn:\n",
    "                    weights = weights[:2]\n",
    "                    inputs = inputs[:2]\n",
    "                if len(weights) < 2:\n",
    "                    continue\n",
    "                features = _make_features(n_example, weights, inputs,\n",
    "                                          toker, args.max_seq_len)\n",
    "                for feature in features:\n",
    "                    chunk.append(vars(feature))\n",
    "                    n_example += 1\n",
    "            except Exception as e:\n",
    "                print('!!! prepro exception !!!', e)\n",
    "                continue\n",
    "        # save last chunk\n",
    "        db[f'chunk_{n_chunk}'] = gzip.compress(\n",
    "            json.dumps(chunk).encode('utf-8'))\n",
    "    # save relevant information to reproduce\n",
    "    meta = {'n_example': n_example,\n",
    "            'chunk_size': args.chunk_size,\n",
    "            'max_seq_len': args.max_seq_len,\n",
    "            'reverse': args.reverse,\n",
    "            'two_turn': args.two_turn}\n",
    "    with open(join(dirname(db_path), 'meta.json'), 'w') as writer:\n",
    "        json.dump(meta, writer, indent=4)\n",
    "    torch.save(toker, join(dirname(db_path), 'tokenizer.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toker = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer=toker\n",
    "text=\"0.0 if you could have one ' do over ' in your life , what would you do differently ?\t1.0 find different ways to fund myself - no matter how desperate , there was always a better way .\"\n",
    "weights, inputs = _get_inputs_from_text(text, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0, 1.0],\n",
       " [[361,\n",
       "   345,\n",
       "   714,\n",
       "   423,\n",
       "   530,\n",
       "   705,\n",
       "   466,\n",
       "   625,\n",
       "   705,\n",
       "   287,\n",
       "   534,\n",
       "   1204,\n",
       "   837,\n",
       "   644,\n",
       "   561,\n",
       "   345,\n",
       "   466,\n",
       "   10338,\n",
       "   5633],\n",
       "  [19796,\n",
       "   1180,\n",
       "   2842,\n",
       "   284,\n",
       "   1814,\n",
       "   3589,\n",
       "   532,\n",
       "   645,\n",
       "   2300,\n",
       "   703,\n",
       "   12111,\n",
       "   837,\n",
       "   612,\n",
       "   373,\n",
       "   1464,\n",
       "   257,\n",
       "   1365,\n",
       "   835,\n",
       "   764]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights,inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_example=0\n",
    "\n",
    "features = _make_features(n_example, weights, inputs,toker, 128)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_id:\t0\n",
      "input_ids:\t[361, 345, 714, 423, 530, 705, 466, 625, 705, 287, 534, 1204, 837, 644, 561, 345, 466, 10338, 5633, 50256, 19796, 1180, 2842, 284, 1814, 3589, 532, 645, 2300, 703, 12111, 837, 612, 373, 1464, 257, 1365, 835, 764, 0]\n",
      "position_ids:\t[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "token_type_ids:\t[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "lm_labels:\t[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 19796, 1180, 2842, 284, 1814, 3589, 532, 645, 2300, 703, 12111, 837, 612, 373, 1464, 257, 1365, 835, 764, 50256, -1]\n",
      "weights:\t[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "features.input_len\n",
    "print('conv_id:',end='\\t')\n",
    "print(features.conv_id)\n",
    "\n",
    "print('input_ids:',end='\\t')\n",
    "print(features.input_ids )\n",
    "\n",
    "print('position_ids:',end='\\t')\n",
    "print(features.position_ids )\n",
    "\n",
    "print('token_type_ids:',end='\\t')\n",
    "print(features.token_type_ids )\n",
    "\n",
    "print('lm_labels:',end='\\t')\n",
    "print(features.lm_labels )\n",
    "\n",
    "print('weights:',end='\\t')\n",
    "print(features.weights )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check this one: \n",
    "\n",
    "#https://github.com/huggingface/transformers/tree/v0.6.2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
